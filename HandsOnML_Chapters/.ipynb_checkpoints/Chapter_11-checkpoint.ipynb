{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When flowing backwards through neural network, sometimes (when applying chain rule) gradients\n",
    "\n",
    "* Grow larger and larger\n",
    "* Grow smaller and smaller\n",
    "\n",
    "This means that different layers can possibly learn at very different learning rates. This is due to\n",
    "\n",
    "* Weight initialization technique\n",
    "* Bad choice of activation function (sigmoid, for example)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Need variance of outputs of each layer to be equal to variance of inputs. \n",
    "2. Need gradients to have equal variance before and after flowing through a layer.\n",
    "\n",
    "There are two ways to achieve this in practice. Let $f_{in}$ by number of input neurons and $f_{out}$ be number of output neurons. Then $f_{avg}=\\frac{1}{2}(f_{in}+f_{out})$. \n",
    "\n",
    "1. Weights chosen by normal ditribution with $\\mu=0$ and $\\sigma^2 = 1/f_{avg}$\n",
    "2. Uniform between -$r$ and $r$ with $r=\\sqrt{3/f_{avg}}$\n",
    "\n",
    "By default Keras uses a uniform distribution. When creating a layer can change this by using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x2057ff7ef28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See table on 334 and code on 335 for more initialization possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally ReLU is not the best, since below $x=0$ its slope is $0$ and thus neurons can \"die\" (aka they don't get tweaked anymore during gradient descent) if they enter this region. The best activation functions are **ELU** and **SELU**. The **ELU** replaces the $0$ part in the negative $x$ axis of the ReLu with a decaying exponential. **SELU** is a scaled variant of **ELU**. **SELU** requires\n",
    "\n",
    "1. Input features must by standardized (mean 0 std 1)\n",
    "2. Hidden layers weights must be initialized with LeCun normal initialization. This means that $\\sigma^2=1/f_{in}$ instead of $1/f_{avg}$.\n",
    "3. Network must be sequential (no fancy stuff with splitting up training set and having some layers skip ahead etc..)\n",
    "\n",
    "For implementing these activation functions see 337-338."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a techinque that zero-centers and normalizes each input (to a neuron) before the activation function. This helps ensure that vanishing/exploding gradient problem doesn't come back during late times in training.\n",
    "\n",
    "See 338-342 for more mathematical details, but know that it essentially \"acts\" as a standard scaler between layers. \n",
    "\n",
    "The reason it is called \"batch\" normalization is because it normalizes entries using\n",
    "\n",
    "$$\\hat{x} = (x-\\mu_B)/\\sigma_B $$\n",
    "\n",
    "where $\\mu_B$ and $\\sigma_B$ (vectors) are computed using only a batch of the data (using full dataset is not practically for stocahstic gradient descent). It then weights them into the neuron using\n",
    "\n",
    "$$ \\text{Input} = \\gamma \\otimes \\hat{x} + \\beta $$\n",
    "\n",
    "and thus $\\gamma$ and $\\beta$ are like the effective neuron weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important for Coding**: Batch normalization can be accomplished as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One parameter to tweak is **momentum** which affects how means or standard deviations are updated from batch to batch. Let $v$ represent $\\mu_B$ or $\\sigma_B$ (moving avg)\n",
    "\n",
    "$$\\hat{v} \\leftarrow \\hat{v} \\times \\text{momentum} + v \\times (1-\\text{momentum}) $$\n",
    "\n",
    "If momentum is 1 (standard) then moving average is just what the current batch, but if not then it retains information from the previous batch (like smoothing function in time series analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after training (when evaluating on test set) the batch layers use $\\mu$ and $\\sigma$ (true values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
