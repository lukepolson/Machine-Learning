{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When flowing backwards through neural network, sometimes (when applying chain rule) gradients\n",
    "\n",
    "* Grow larger and larger\n",
    "* Grow smaller and smaller\n",
    "\n",
    "This means that different layers can possibly learn at very different learning rates. This is due to\n",
    "\n",
    "* Weight initialization technique\n",
    "* Bad choice of activation function (sigmoid, for example)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Need variance of outputs of each layer to be equal to variance of inputs. \n",
    "2. Need gradients to have equal variance before and after flowing through a layer.\n",
    "\n",
    "There are two ways to achieve this in practice. Let $f_{in}$ by number of input neurons and $f_{out}$ be number of output neurons. Then $f_{avg}=\\frac{1}{2}(f_{in}+f_{out})$. \n",
    "\n",
    "1. Weights chosen by normal ditribution with $\\mu=0$ and $\\sigma^2 = 1/f_{avg}$\n",
    "2. Uniform between -$r$ and $r$ with $r=\\sqrt{3/f_{avg}}$\n",
    "\n",
    "By default Keras uses a uniform distribution. When creating a layer can change this by using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x211b3f0fc50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See table on 334 and code on 335 for more initialization possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally ReLU is not the best, since below $x=0$ its slope is $0$ and thus neurons can \"die\" (aka they don't get tweaked anymore during gradient descent) if they enter this region. The best activation functions are **ELU** and **SELU**. The **ELU** replaces the $0$ part in the negative $x$ axis of the ReLu with a decaying exponential. **SELU** is a scaled variant of **ELU**. **SELU** requires\n",
    "\n",
    "1. Input features must by standardized (mean 0 std 1)\n",
    "2. Hidden layers weights must be initialized with LeCun normal initialization. This means that $\\sigma^2=1/f_{in}$ instead of $1/f_{avg}$.\n",
    "3. Network must be sequential (no fancy stuff with splitting up training set and having some layers skip ahead etc..)\n",
    "\n",
    "For implementing these activation functions see 337-338."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a techinque that zero-centers and normalizes each input (to a neuron) before the activation function. This helps ensure that vanishing/exploding gradient problem doesn't come back during late times in training.\n",
    "\n",
    "See 338-342 for more mathematical details, but know that it essentially \"acts\" as a standard scaler between layers. \n",
    "\n",
    "The reason it is called \"batch\" normalization is because it normalizes entries using\n",
    "\n",
    "$$\\hat{x} = (x-\\mu_B)/\\sigma_B $$\n",
    "\n",
    "where $\\mu_B$ and $\\sigma_B$ (vectors) are computed using only a batch of the data (using full dataset is not practically for stocahstic gradient descent). It then weights them into the neuron using\n",
    "\n",
    "$$ \\text{Input} = \\gamma \\otimes \\hat{x} + \\beta $$\n",
    "\n",
    "and thus $\\gamma$ and $\\beta$ are like the effective neuron weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important for Coding**: Batch normalization can be accomplished as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One parameter to tweak is **momentum** which affects how means or standard deviations are updated from batch to batch. Let $v$ represent $\\mu_B$ or $\\sigma_B$ (moving avg)\n",
    "\n",
    "$$\\hat{v} \\leftarrow \\hat{v} \\times \\text{momentum} + v \\times (1-\\text{momentum}) $$\n",
    "\n",
    "If momentum is 1 (standard) then moving average is just what the current batch, but if not then it retains information from the previous batch (like smoothing function in time series analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after training (when evaluating on test set) the batch layers use $\\mu$ and $\\sigma$ (true values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For certain neural networks called *recurrent neural networks* batch normalization is tricky to use. As such other technqiues have been developed to deal with exploding gradients. One technique is gradient clipping, where large gradients are clipped between a given range (never exceed some value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all gradients will be clipped between -1.0 and 1.0. What this means, however, is if a gradient is initially [-0.9, 100] it will be clipped to [-0.9,1] (direction changes by quite a bit). In practice it still works well though. To avoid, this, you can use the \"clipnorm\" argument which instead normalizes by the length of the vector. Generally one should use both approaches and see which one works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally not a good idea to trian DNN from scratch: reuse some layers from another network. This is called **transfer learning**. Generally\n",
    "\n",
    "* Low layers far more useful than upper layers \n",
    "\n",
    "Procedure to do this:\n",
    "\n",
    "1. Select layers want to use\n",
    "2. Freeze them\n",
    "3. Add new upper layers\n",
    "4. Train upper layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do this on fashion MNIST data. Suppose we have\n",
    "\n",
    "* 200 images of shirts and sandals we want to classify\n",
    "* Model from fashion MNIST which used way more data to train\n",
    "\n",
    "We could just train a model on the 200 images but because training sample is small, wouldn't get good accuracy. Instea we can use model from full MNIST data and us it for the shirt sandal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train on full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/20\n",
      "43986/43986 [==============================] - 4s 98us/sample - loss: 0.5909 - accuracy: 0.8100 - val_loss: 0.3772 - val_accuracy: 0.8705\n",
      "Epoch 2/20\n",
      "43986/43986 [==============================] - 3s 79us/sample - loss: 0.3520 - accuracy: 0.8793 - val_loss: 0.3398 - val_accuracy: 0.8774\n",
      "Epoch 3/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.3167 - accuracy: 0.8892 - val_loss: 0.3018 - val_accuracy: 0.8956\n",
      "Epoch 4/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2969 - accuracy: 0.8969 - val_loss: 0.2858 - val_accuracy: 0.9026\n",
      "Epoch 5/20\n",
      "43986/43986 [==============================] - 3s 79us/sample - loss: 0.2824 - accuracy: 0.9029 - val_loss: 0.2807 - val_accuracy: 0.9063\n",
      "Epoch 6/20\n",
      "43986/43986 [==============================] - 3s 78us/sample - loss: 0.2721 - accuracy: 0.9072 - val_loss: 0.2684 - val_accuracy: 0.9103\n",
      "Epoch 7/20\n",
      "43986/43986 [==============================] - 3s 78us/sample - loss: 0.2642 - accuracy: 0.9096 - val_loss: 0.2678 - val_accuracy: 0.9103\n",
      "Epoch 8/20\n",
      "43986/43986 [==============================] - 3s 78us/sample - loss: 0.2571 - accuracy: 0.9116 - val_loss: 0.2740 - val_accuracy: 0.9053\n",
      "Epoch 9/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2514 - accuracy: 0.9140 - val_loss: 0.2588 - val_accuracy: 0.9116\n",
      "Epoch 10/20\n",
      "43986/43986 [==============================] - 3s 79us/sample - loss: 0.2456 - accuracy: 0.9164 - val_loss: 0.2600 - val_accuracy: 0.9108\n",
      "Epoch 11/20\n",
      "43986/43986 [==============================] - 3s 79us/sample - loss: 0.2416 - accuracy: 0.9178 - val_loss: 0.2486 - val_accuracy: 0.9163\n",
      "Epoch 12/20\n",
      "43986/43986 [==============================] - 3s 78us/sample - loss: 0.2379 - accuracy: 0.9187 - val_loss: 0.2449 - val_accuracy: 0.9158\n",
      "Epoch 13/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2343 - accuracy: 0.9204 - val_loss: 0.2455 - val_accuracy: 0.9160\n",
      "Epoch 14/20\n",
      "43986/43986 [==============================] - 3s 79us/sample - loss: 0.2308 - accuracy: 0.9207 - val_loss: 0.2438 - val_accuracy: 0.9168\n",
      "Epoch 15/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2278 - accuracy: 0.9216 - val_loss: 0.2394 - val_accuracy: 0.9183\n",
      "Epoch 16/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2250 - accuracy: 0.9227 - val_loss: 0.2392 - val_accuracy: 0.9183\n",
      "Epoch 17/20\n",
      "43986/43986 [==============================] - 3s 80us/sample - loss: 0.2218 - accuracy: 0.9238 - val_loss: 0.2387 - val_accuracy: 0.9180\n",
      "Epoch 18/20\n",
      "43986/43986 [==============================] - 4s 80us/sample - loss: 0.2199 - accuracy: 0.9243 - val_loss: 0.2398 - val_accuracy: 0.9178\n",
      "Epoch 19/20\n",
      "43986/43986 [==============================] - 3s 78us/sample - loss: 0.2177 - accuracy: 0.9252 - val_loss: 0.2331 - val_accuracy: 0.9205\n",
      "Epoch 20/20\n",
      "43986/43986 [==============================] - 3s 78us/sample - loss: 0.2154 - accuracy: 0.9255 - val_loss: 0.2326 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "# Compile\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"chap11_my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train model on the 200 Binary Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.9545 - accuracy: 0.4600 - val_loss: 0.6655 - val_accuracy: 0.5385\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 0s 424us/sample - loss: 0.5899 - accuracy: 0.6900 - val_loss: 0.4785 - val_accuracy: 0.8519\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 0s 409us/sample - loss: 0.4512 - accuracy: 0.8800 - val_loss: 0.4098 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 0s 399us/sample - loss: 0.3871 - accuracy: 0.9100 - val_loss: 0.3666 - val_accuracy: 0.9128\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 0s 374us/sample - loss: 0.3438 - accuracy: 0.9250 - val_loss: 0.3315 - val_accuracy: 0.9300\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 0s 414us/sample - loss: 0.3095 - accuracy: 0.9300 - val_loss: 0.3034 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 0s 389us/sample - loss: 0.2810 - accuracy: 0.9400 - val_loss: 0.2808 - val_accuracy: 0.9432\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 0s 364us/sample - loss: 0.2580 - accuracy: 0.9500 - val_loss: 0.2618 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 0s 354us/sample - loss: 0.2372 - accuracy: 0.9600 - val_loss: 0.2447 - val_accuracy: 0.9513\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 0s 369us/sample - loss: 0.2196 - accuracy: 0.9650 - val_loss: 0.2316 - val_accuracy: 0.9513\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 0s 394us/sample - loss: 0.2048 - accuracy: 0.9650 - val_loss: 0.2182 - val_accuracy: 0.9533\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 0s 399us/sample - loss: 0.1915 - accuracy: 0.9650 - val_loss: 0.2071 - val_accuracy: 0.9564\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 0s 379us/sample - loss: 0.1791 - accuracy: 0.9650 - val_loss: 0.1959 - val_accuracy: 0.9594\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.1681 - accuracy: 0.9700 - val_loss: 0.1864 - val_accuracy: 0.9604\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 0s 374us/sample - loss: 0.1579 - accuracy: 0.9850 - val_loss: 0.1778 - val_accuracy: 0.9625\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 0s 394us/sample - loss: 0.1491 - accuracy: 0.9850 - val_loss: 0.1695 - val_accuracy: 0.9675\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.1411 - accuracy: 0.9900 - val_loss: 0.1626 - val_accuracy: 0.9686\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 0s 354us/sample - loss: 0.1339 - accuracy: 0.9900 - val_loss: 0.1570 - val_accuracy: 0.9686\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 0s 394us/sample - loss: 0.1277 - accuracy: 0.9900 - val_loss: 0.1522 - val_accuracy: 0.9696\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 0s 374us/sample - loss: 0.1217 - accuracy: 0.9900 - val_loss: 0.1471 - val_accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "# Build\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have two models: our \"shitty one\" trained on only 200 images but we also have a much more powerful model trained on a bigger data set (but it doesn't quite do what we want it to do- its not a binary classifier). How can we use parts of that model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transfer Learning: Reuse parts of good model for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets strip model A of its final layer (softmax) and replace it with a binary classifier layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get good model\n",
    "model_A = keras.models.load_model(\"chap11_my_model_A.h5\")\n",
    "# Use every layer except last one\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "# Make a sigmoid final layer for binary classification\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training this will actually affect model A, so we need to clone it if we also want to keep a copy of model A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we freeze the reused layers to allow the final layer to adapt (since its weights are initially randomized and it will take some time to train them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze weights\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train it for a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 1s 3ms/sample - loss: 0.5770 - accuracy: 0.6550 - val_loss: 0.5778 - val_accuracy: 0.6450\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.5391 - accuracy: 0.7000 - val_loss: 0.5435 - val_accuracy: 0.6836\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 369us/sample - loss: 0.5053 - accuracy: 0.7200 - val_loss: 0.5132 - val_accuracy: 0.7150\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 369us/sample - loss: 0.4753 - accuracy: 0.7600 - val_loss: 0.4839 - val_accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now unfreeze layers, compile again, and train some more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 1s 3ms/sample - loss: 0.3942 - accuracy: 0.8250 - val_loss: 0.3372 - val_accuracy: 0.8722\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 0s 404us/sample - loss: 0.2742 - accuracy: 0.9300 - val_loss: 0.2605 - val_accuracy: 0.9280\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 0s 394us/sample - loss: 0.2103 - accuracy: 0.9650 - val_loss: 0.2148 - val_accuracy: 0.9523\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 0s 399us/sample - loss: 0.1707 - accuracy: 0.9750 - val_loss: 0.1803 - val_accuracy: 0.9635\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 0s 369us/sample - loss: 0.1412 - accuracy: 0.9800 - val_loss: 0.1573 - val_accuracy: 0.9757\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 0s 379us/sample - loss: 0.1212 - accuracy: 0.9900 - val_loss: 0.1394 - val_accuracy: 0.9807\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.1056 - accuracy: 0.9950 - val_loss: 0.1260 - val_accuracy: 0.9828\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 0s 414us/sample - loss: 0.0935 - accuracy: 1.0000 - val_loss: 0.1154 - val_accuracy: 0.9848\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 0s 354us/sample - loss: 0.0840 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9858\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.0766 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 0.9868\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.0706 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 0s 394us/sample - loss: 0.0656 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9899\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 0s 369us/sample - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9899\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.0567 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9899\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 0s 374us/sample - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9899\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 0s 364us/sample - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion: Compare model trained on 200 images to reused model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model B (shitty one trained on 200 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "B_results = model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14446661925315857, 0.9695]\n"
     ]
    }
   ],
   "source": [
    "print(B_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model that reused some layers of big model trained on lots of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "BonA_results = model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06893341457843781, 0.9925]\n"
     ]
    }
   ],
   "source": [
    "print(BonA_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy went up by quite a bit! **HOWEVER**, this only worked because **of the random seed used**. This is called \"torturing the data until it confesses\". Some random seeds don't even show any improvement!\n",
    "\n",
    "**Moral of the Story**: If a flashy new paper looks too positive, be suspicious, the new technique might not actually be as good as the author says it is. The authors might have tried manby variants until they got one that looked successful, without mentioning their many failures. It turns out that transfer learning does not work well with small dense networks, since they learn very few patterns and very specific patterns. Transfer learning works best with deep convolutional networks. **Transfer learning will be revisisted properly in chapter 14**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since labels are generally expensive to obtain in the real world, but training data is less expensive, one typically does unsupervised pretraining on many cases without labels, and then at the very end (with a small set of labeled data) one reuses the layers of this pretrained network and trains on the few cases of labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Building a system to classify images but only have a few pictures of people. Pretrain using many images on google to tell if pictures contain same person, then reuse this network for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at techniques other than the regular SGD method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change update structure to\n",
    "\n",
    "1. $\\mathbf{m} \\to \\beta \\mathbf{m} -\\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{\\theta} \\to \\mathbf{\\theta}+\\mathbf{m}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is like momentum in physics. Easy to verify that constant gradient (flat downward slope) yields max momentum proportional to $1/(1+\\beta)$. The purpose of this is to speed up gradient descent in flat patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change above to \n",
    "\n",
    "1. $\\mathbf{m} \\to \\beta \\mathbf{m} -\\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}+\\beta \\mathbf{m})$\n",
    "2. $\\mathbf{\\theta} \\to \\mathbf{\\theta}+\\mathbf{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only difference is gradient is calculated slightly ahead of present position. This only works because momentum tends to point in the right direction (momentum is an accumulated avg of overall traveling and tends to point towards optimum).\n",
    "\n",
    "Both of the two procedures can be used in keras as follows (momentum is value of $\\beta$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm decays the learning rate but does so faster for steep dimensions than shallow dimnesions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{s} \\to \\mathbf{s} +\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{\\theta} \\to \\mathbf{\\theta}-\\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\oslash \\sqrt{\\mathbf{s}+\\epsilon} $\n",
    "\n",
    "Note the elementwise multiply and divide symbols. $\\epsilon$ is just there to ensure no division by 0.\n",
    "\n",
    "This algorithm does not work well for neural networks usually because it stops to early. But it is important to understand for the upcoming algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixes adagrad by accumulating only the gradients of the few most recent operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{s} \\to \\beta \\mathbf{s} + (1-\\beta)\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{\\theta} \\to \\mathbf{\\theta}-\\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\oslash \\sqrt{\\mathbf{s}+\\epsilon} $\n",
    "\n",
    "$\\beta$ is typically set to 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam and Nadam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{m} \\to \\beta_1 \\mathbf{m} -(1-\\beta_1)\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "2. $\\mathbf{s} \\to \\beta_2 \\mathbf{s} + (1-\\beta_2)\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "3. $\\hat{\\mathbf{m}} \\to (1-\\beta_1^T)^{-1} \\hat{\\mathbf{m}}  $\n",
    "4. $\\hat{\\mathbf{s}} \\to (1-\\beta_2^T)^{-1} \\hat{\\mathbf{s}}  $\n",
    "5. $\\mathbf{\\theta} \\to \\mathbf{\\theta} + \\eta \\hat{\\mathbf{m}} \\oslash \\sqrt{\\hat{\\mathbf{s}}+\\epsilon}$\n",
    "\n",
    "$ T$ is the iteration number (starting at 1). Steps 3 and 4 are included because $m$ and $s$ are initialized at zero so it helps boost them. $\\beta_1$ is typically 0.9 while $\\beta_2$ is typically 0.999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadam is just Adam with the Nesterov trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See table 11-2 for good summary of all optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to deal with learning rate parameter $\\eta$ and how it changes during epochs. 360-361 goes over a few techniques for decreasing learning rate over time but they all have the same structure:\n",
    "\n",
    "**Start with high learning rate and generally decrease over time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $s$ steps $\\eta \\to \\eta_0/2$. After $s$ more steps $\\eta$ goes to $\\eta_0/3$. Continues to /4, /5, etc. Hyperparameter \"decay\" is the inverse of $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specified (Exponential Decay Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you want learning rate to decay in some user specified way. Need to create a function that returns decay as a function of epoch. Easiest way of doing this (with multiple parameters) is a function of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0*0.1**(epoch/s)\n",
    "    return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to use this as a callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 0s 389us/sample - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9899\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 379us/sample - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9909\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 384us/sample - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 0.9909\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 359us/sample - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B),\n",
    "                           callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also update every step (rather than every epoch). See Geron notebook for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: When you reload a model the epoch starts at 0 when you fit it again. This can screw up the fitting since the learning rate will also start at the initial value. Way of getting around this is to store epoch information as well and use the \"initial_epoch\" argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decreasing Learning Rate when no Improvement is seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new scheduler to decrease learning rate when njo improvement is seen in validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one multiplies lr by 0.5 when no improvement is seen after 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $l_1$ and $l_2$ normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is added to layers. As before, $l_1$ creates sparser models (more weights equal to zero) than $l_2$ normalization. This can be justified through\n",
    "\n",
    "$l_2$ Norm:\n",
    "$\\sqrt{1^2+9^2} = 9.06$\n",
    "\n",
    "whereas\n",
    "\n",
    "$l_1$ Norm:\n",
    "$1+9 = 10$\n",
    "\n",
    "The 1 contributes far more to the $l_1$ norm (second equation) than the $l_2$ norm (first equation) so it tends to be eliminated more frequently if it is less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you typically use the same activation functions, initializers, and regulaizers for all layers it is typically a good idea to create NNs in loops. Another option is to use the \"partial\" library of python which allows you to create a wrapper for any function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Create Wrapper\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                          activation='elu',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0))\n",
    "\n",
    "# Create model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the arguments in the wrapper are set to default values but can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique of dropout is that during each iteration, some neurons \"turn-off\" and are not changed during that iteration. This technique adds a huge accuracy boost to most models. The hyperparameter for this is \"p\" and is usually set between 10 and 15 percent.\n",
    "\n",
    "However, since some neurons are dropped, the sum of the weights $\\sum w_ix_i$ used as input to a neuron decreases. As such, all the neuron weights are multiplied by $1-p$ after training.\n",
    "\n",
    "It is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "55000/55000 [==============================] - 8s 152us/sample - loss: 0.5827 - accuracy: 0.7995 - val_loss: 0.3678 - val_accuracy: 0.8686\n",
      "Epoch 2/5\n",
      "55000/55000 [==============================] - 8s 138us/sample - loss: 0.4234 - accuracy: 0.8442 - val_loss: 0.3456 - val_accuracy: 0.8666\n",
      "Epoch 3/5\n",
      "55000/55000 [==============================] - 8s 144us/sample - loss: 0.3933 - accuracy: 0.8546 - val_loss: 0.3350 - val_accuracy: 0.8748\n",
      "Epoch 4/5\n",
      "55000/55000 [==============================] - 8s 138us/sample - loss: 0.3752 - accuracy: 0.8612 - val_loss: 0.3232 - val_accuracy: 0.8838\n",
      "Epoch 5/5\n",
      "55000/55000 [==============================] - 8s 140us/sample - loss: 0.3639 - accuracy: 0.8651 - val_loss: 0.3158 - val_accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    RegularizedDense(300),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    RegularizedDense(100),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    RegularizedDense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 5\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If a model is overfitting it is usually a good idea to increase the dropout rate**.\n",
    "\n",
    "**For SELU activation functions use alpha dropout- look it up!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way Monte Carlo dropout works is that you make predictions on the test set while the dropout layer is active, then you take the average of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 10 predictions for each of 100 test samples (100 different Monte Carlo Dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer flatten_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the models prediction (with training=False) to a few of these probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the model\n",
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.16, 0.  , 0.79]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monte Carlo Dropout Mean\n",
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.18, 0.  , 0.17]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monte Carlo Dropout STD\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is some standard deviation in the estimates obtained through dropout. These uncertain predictions should be treated with extreme caution in highly sensitive systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo Dropout\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8672\n"
     ]
    }
   ],
   "source": [
    "# Regular Model\n",
    "y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is pretty close between the two (hard to compare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this case we set training equal to true when getting test samples. But what if there are other layers we don't want turned on?** In this case override the MCDropout layer and force training equal to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this class instead when deriving layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.Dropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
